
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>🔹 Principal Component Analysis (PCA) &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css?v=6644e6bb" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Mathematics for Data Science/Statistics/n.PCA_standardaization_and normalization';</script>
    <link rel="icon" href="../../_static/logo.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../introduction.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="My sample book - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="My sample book - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../table_of_contents.html">Table of Contents</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/edit/master/docs/Mathematics for Data Science/Statistics/n.PCA_standardaization_and normalization.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FMathematics for Data Science/Statistics/n.PCA_standardaization_and normalization.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/Mathematics for Data Science/Statistics/n.PCA_standardaization_and normalization.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>🔹 Principal Component Analysis (PCA)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">🔹 Principal Component Analysis (PCA)</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#standardization-vs-normalization-understanding-their-impact-with-plots">Standardization vs. Normalization: Understanding Their Impact with Plots</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-feature-scaling">Introduction to Feature Scaling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#standardization-z-score-scaling">1. Standardization (Z-score Scaling)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization-min-max-scaling">2. Normalization (Min-Max Scaling)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-differences-visual-impact">Key Differences &amp; Visual Impact</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <p>You’ve requested a detailed explanation of Principal Component Analysis (PCA) with plots. I’ve previously provided this in our conversation as part of the “Multivariate &amp; Dimensionality Techniques” section.</p>
<p>Here’s the detailed breakdown of Principal Component Analysis (PCA) again, with mathematical intuition, a problem statement, and illustrative plots:</p>
<hr class="docutils" />
<section id="principal-component-analysis-pca">
<h1>🔹 Principal Component Analysis (PCA)<a class="headerlink" href="#principal-component-analysis-pca" title="Link to this heading">#</a></h1>
<p><strong>Explanation:</strong>
Principal Component Analysis (PCA) is a widely used <strong>dimensionality reduction technique</strong>. Its main goal is to transform a large set of correlated variables into a smaller set of uncorrelated variables called <strong>Principal Components (PCs)</strong>. These new components capture the most variance in the data. The first PC captures the most variance, the second PC captures the second most variance (and is orthogonal to the first), and so on.</p>
<p><strong>Purpose:</strong></p>
<ul class="simple">
<li><p><strong>Dimensionality Reduction:</strong> Reduce the number of features while retaining most of the information. This is crucial for high-dimensional datasets where many features might be redundant or highly correlated.</p></li>
<li><p><strong>Noise Reduction:</strong> By focusing on major components, less important variance (often considered noise) is typically discarded. This can lead to cleaner data for subsequent analysis.</p></li>
<li><p><strong>Data Visualization:</strong> It can reduce high-dimensional data to 2 or 3 dimensions, making it possible to plot and visually explore patterns, clusters, or outliers that were otherwise hidden.</p></li>
<li><p><strong>Feature Engineering:</strong> The Principal Components themselves can be used as new, independent features for other machine learning models (e.g., in regression, classification, or clustering), often leading to improved model performance and reduced overfitting.</p></li>
</ul>
<p><strong>Mathematical Intuition:</strong>
PCA essentially finds the directions (or axes) along which the data varies the most. These directions are the eigenvectors of the covariance matrix of your data.</p>
<ol class="arabic">
<li><p><strong>Covariance Matrix:</strong></p>
<ul class="simple">
<li><p>The covariance matrix is a square matrix that describes the variance of each variable and the covariance between each pair of variables.</p></li>
<li><p>For a dataset with <span class="math notranslate nohighlight">\(p\)</span> features, the covariance matrix <span class="math notranslate nohighlight">\(\\Sigma\)</span> will be a <span class="math notranslate nohighlight">\(p \\times p\)</span> symmetric matrix.</p></li>
<li><p>Each diagonal element <span class="math notranslate nohighlight">\(\\Sigma\_{ii}\)</span> represents the variance of feature <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
<li><p>Each off-diagonal element <span class="math notranslate nohighlight">\(\\Sigma\_{ij}\)</span> represents the covariance between feature <span class="math notranslate nohighlight">\(i\)</span> and feature <span class="math notranslate nohighlight">\(j\)</span>.</p></li>
<li><p>A high positive covariance indicates that two variables tend to increase or decrease together. A high negative covariance indicates that as one variable increases, the other tends to decrease. If the covariance is near zero, it suggests little linear relationship between the variables.</p></li>
</ul>
<p><em>Example for 2 variables (<span class="math notranslate nohighlight">\(X\_1, X\_2\)</span>):</em>
$<span class="math notranslate nohighlight">\(\Sigma = \begin{pmatrix} \text{Var}(X_1) &amp; \text{Cov}(X_1, X_2) \\ \text{Cov}(X_2, X_1) &amp; \text{Var}(X_2) \end{pmatrix}\)</span>$
The goal of PCA is to decorrelate the variables, so that in the new PC space, the covariance matrix becomes diagonal (all off-diagonal elements become zero).</p>
</li>
<li><p><strong>Eigenvalues and Eigenvectors:</strong></p>
<ul class="simple">
<li><p><strong>Eigenvectors:</strong> These are special non-zero vectors that, when a linear transformation (like multiplying by the covariance matrix) is applied to them, only change by a scalar factor. They are fundamental to PCA because they represent the <strong>principal components</strong>. Each eigenvector defines a new axis in the data space. These axes are orthogonal (perpendicular) to each other, meaning they capture independent dimensions of variance.</p></li>
<li><p><strong>Eigenvalues:</strong> The scalar factor by which an eigenvector is scaled is its corresponding eigenvalue. In PCA, eigenvalues quantify the <strong>amount of variance</strong> explained by each principal component. A larger eigenvalue means its corresponding eigenvector (principal component) captures more of the total variance in the data. The eigenvector with the largest eigenvalue is the first principal component, the one with the second largest eigenvalue is the second principal component, and so on.</p></li>
</ul>
<p>The core mathematical problem of PCA is to find the eigenvalues <span class="math notranslate nohighlight">\(\\lambda\)</span> and eigenvectors <span class="math notranslate nohighlight">\(\\mathbf{v}\)</span> for a covariance matrix <span class="math notranslate nohighlight">\(\\Sigma\)</span> such that:
$<span class="math notranslate nohighlight">\(\Sigma \mathbf{v} = \lambda \mathbf{v}\)</span>$</p>
</li>
<li><p><strong>Steps of PCA:</strong></p>
<ol class="arabic simple">
<li><p><strong>Standardize the Data:</strong> (Crucial for most applications) If your variables have different scales (e.g., one measured in dollars, another in years), standardize them to have zero mean and unit variance. This prevents variables with larger scales from dominating the principal components, ensuring that variance is given equal importance across all features.
$<span class="math notranslate nohighlight">\(z = \frac{x - \mu}{\sigma}\)</span>$</p></li>
<li><p><strong>Compute the Covariance Matrix:</strong> Calculate the covariance matrix of the standardized data. This matrix reveals the relationships (covariances) between all pairs of your features.</p></li>
<li><p><strong>Calculate Eigenvalues and Eigenvectors:</strong> Perform eigen decomposition on the covariance matrix. This mathematical operation yields the set of eigenvalues and their corresponding eigenvectors.</p></li>
<li><p><strong>Sort Eigenvalues:</strong> Order the eigenvalues from largest to smallest. The corresponding eigenvectors are now the principal components, ordered by the amount of variance they explain.</p></li>
<li><p><strong>Select Principal Components:</strong> Choose a subset of principal components (eigenvectors) corresponding to the largest eigenvalues. The number of components to retain can be decided based on:</p>
<ul class="simple">
<li><p><strong>Explained Variance Threshold:</strong> Retain enough components to explain a certain percentage of the total variance (e.g., 90% or 95%).</p></li>
<li><p><strong>Scree Plot:</strong> (See below) Look for an “elbow” in the plot.</p></li>
<li><p><strong>Fixed Number:</strong> Choose a specific number (e.g., 2 or 3 for visualization).</p></li>
</ul>
</li>
<li><p><strong>Transform Data:</strong> Project the original (standardized) data onto the selected principal components. This creates a new, lower-dimensional representation of your data where the new features (PCs) are uncorrelated.</p></li>
</ol>
</li>
</ol>
<p><strong>Scree Plot:</strong>
A <strong>Scree plot</strong> is a line plot of the eigenvalues (or the proportion of variance explained) of principal components in descending order. It’s a visual tool used to help determine the “optimal” number of components to retain. You typically look for an “elbow” point in the plot where the slope of the line dramatically changes (becomes much flatter). This suggests that components after this point contribute much less significantly to explaining the total variance and might represent noise rather than meaningful structure.</p>
<p><strong>Problem Statement:</strong></p>
<ul class="simple">
<li><p><strong>Problem:</strong> A retail company has collected data on 10 different customer engagement metrics (e.g., website visits, average time on site, number of purchases, items viewed, newsletter clicks, customer service interactions, bounce rate, etc.). These metrics are often highly correlated. They want to understand the underlying patterns of customer engagement, reduce the dimensionality of this data to simplify analysis, and potentially build a more robust customer segmentation model, as having 10 correlated features is complex for visualization and modeling.</p></li>
<li><p><strong>PCA Application:</strong> PCA can identify the few principal components that capture most of the variation in these 10 metrics. For example, PC1 might represent “overall customer activity,” PC2 might represent “engagement with promotional content,” and PC3 “customer service dependency.” By reducing the data to 2 or 3 principal components, the company can visualize customer segments more easily, or use these new uncorrelated features in subsequent clustering or predictive models, potentially improving performance and interpretability.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="kn">import</span> <span class="n">PCA</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">&quot;whitegrid&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.family&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;sans-serif&#39;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.sans-serif&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;DejaVu Sans&#39;</span><span class="p">]</span> <span class="c1"># Or your preferred sans-serif font</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;--- Principal Component Analysis (PCA) ---&quot;</span><span class="p">)</span>

<span class="c1"># 1. Simulate some correlated data (e.g., customer metrics)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span> <span class="c1"># For reproducibility</span>
<span class="n">data_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">num_features</span> <span class="o">=</span> <span class="mi">5</span> <span class="c1"># Simulating 5 customer engagement metrics</span>

<span class="c1"># Create features with varying degrees of correlation</span>
<span class="c1"># Feature 1 (Base Activity)</span>
<span class="n">feature1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">data_size</span><span class="p">)</span>
<span class="c1"># Feature 2 (Highly correlated with Feature 1, e.g., &#39;Time on Site&#39; vs &#39;Website Visits&#39;)</span>
<span class="n">feature2</span> <span class="o">=</span> <span class="n">feature1</span> <span class="o">*</span> <span class="mf">0.8</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">data_size</span><span class="p">)</span>
<span class="c1"># Feature 3 (Another base, e.g., &#39;Newsletter Clicks&#39;)</span>
<span class="n">feature3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">data_size</span><span class="p">)</span>
<span class="c1"># Feature 4 (Correlated with Feature 3, e.g., &#39;Promotion Conversions&#39; vs &#39;Newsletter Clicks&#39;)</span>
<span class="n">feature4</span> <span class="o">=</span> <span class="n">feature3</span> <span class="o">*</span> <span class="mf">0.6</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">data_size</span><span class="p">)</span>
<span class="c1"># Feature 5 (Less correlated, e.g., &#39;Customer Service Calls&#39;)</span>
<span class="n">feature5</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">70</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="n">data_size</span><span class="p">)</span> 

<span class="n">df_pca</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;Website_Visits&#39;</span><span class="p">:</span> <span class="n">feature1</span><span class="p">,</span>
    <span class="s1">&#39;Time_On_Site&#39;</span><span class="p">:</span> <span class="n">feature2</span><span class="p">,</span>
    <span class="s1">&#39;Newsletter_Clicks&#39;</span><span class="p">:</span> <span class="n">feature3</span><span class="p">,</span>
    <span class="s1">&#39;Promo_Conversions&#39;</span><span class="p">:</span> <span class="n">feature4</span><span class="p">,</span>
    <span class="s1">&#39;Customer_Service_Calls&#39;</span><span class="p">:</span> <span class="n">feature5</span>
<span class="p">})</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original Data Head (First 5 rows):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df_pca</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Original Data Covariance Matrix (demonstrates correlations):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df_pca</span><span class="o">.</span><span class="n">cov</span><span class="p">())</span>

<span class="c1"># 2. Standardize the data</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaled_data</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df_pca</span><span class="p">)</span>
<span class="n">scaled_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scaled_data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">df_pca</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Scaled Data Head (First 5 rows):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">scaled_df</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Scaled Data Covariance Matrix (demonstrates correlations on standardized scale):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">scaled_df</span><span class="o">.</span><span class="n">cov</span><span class="p">())</span> <span class="c1"># Now variances are ~1, but covariances (correlations) still exist</span>

<span class="c1"># 3. Perform PCA</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> <span class="c1"># Keep all components initially to analyze variance</span>
<span class="n">pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">scaled_data</span><span class="p">)</span>

<span class="c1"># Explained variance ratio</span>
<span class="n">explained_variance_ratio</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Explained Variance Ratio by each Principal Component: </span><span class="si">{</span><span class="n">explained_variance_ratio</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cumulative Explained Variance: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">explained_variance_ratio</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># --- Plotting: Scree Plot ---</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">explained_variance_ratio</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">explained_variance_ratio</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;purple&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Scree Plot (Explained Variance by Principal Component)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Principal Component Number&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Proportion of Variance Explained&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">explained_variance_ratio</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># --- Plotting: Cumulative Explained Variance ---</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">explained_variance_ratio</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">explained_variance_ratio</span><span class="p">),</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;darkorange&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mf">0.90</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;90</span><span class="si">% E</span><span class="s1">xplained Variance&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Cumulative Explained Variance&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of Principal Components&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Cumulative Proportion of Variance Explained&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">explained_variance_ratio</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Project data onto the first 2 principal components for visualization</span>
<span class="c1"># Based on the scree plot, we might choose 2 or 3 components if they explain enough variance.</span>
<span class="c1"># Here, we project to 2D for easy visualization.</span>
<span class="n">pca_2d</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">principal_components</span> <span class="o">=</span> <span class="n">pca_2d</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">scaled_data</span><span class="p">)</span>
<span class="n">df_pca_2d</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">principal_components</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Principal Component 1&#39;</span><span class="p">,</span> <span class="s1">&#39;Principal Component 2&#39;</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Transformed Data (First 2 Principal Components) Head:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df_pca_2d</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>

<span class="c1"># --- Plotting: Data Projected onto First Two Principal Components ---</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_pca_2d</span><span class="p">[</span><span class="s1">&#39;Principal Component 1&#39;</span><span class="p">],</span> <span class="n">df_pca_2d</span><span class="p">[</span><span class="s1">&#39;Principal Component 2&#39;</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;skyblue&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Data Projected onto First Two Principal Components&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Principal Component 1 (</span><span class="si">{</span><span class="n">explained_variance_ratio</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">% variance)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Principal Component 2 (</span><span class="si">{</span><span class="n">explained_variance_ratio</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">% variance)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># --- Display Eigenvectors (Loadings) for the first two components ---</span>
<span class="c1"># These show how much each original feature contributes to each principal component.</span>
<span class="c1"># The sign indicates direction of correlation.</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Eigenvectors (Loadings) for the first 2 Principal Components:&quot;</span><span class="p">)</span>
<span class="c1"># Each column is a PC, each row is an original feature</span>
<span class="n">loadings_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">pca_2d</span><span class="o">.</span><span class="n">components_</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;PC1&#39;</span><span class="p">,</span> <span class="s1">&#39;PC2&#39;</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="n">df_pca</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">loadings_df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>--- Principal Component Analysis (PCA) ---
Original Data Head (First 5 rows):
   Website_Visits  Time_On_Site  Newsletter_Clicks  Promo_Conversions  \
0       54.967142     36.896860          32.862299          16.401399   
1       48.617357     36.790659          34.486276          18.451042   
2       56.476885     43.467936          38.664410          26.187820   
3       65.230299     48.172853          38.430416          25.499731   
4       47.658466     37.320344          18.978645          11.303581   

   Customer_Service_Calls  
0               50.866868  
1               62.807500  
2               70.062924  
3               70.563767  
4               64.599214  

Original Data Covariance Matrix (demonstrates correlations):
                        Website_Visits  Time_On_Site  Newsletter_Clicks  \
Website_Visits               82.476989     60.073882          15.033780   
Time_On_Site                 60.073882     66.070050          10.511866   
Newsletter_Clicks            15.033780     10.511866          75.242840   
Promo_Conversions             3.553174      1.636443          45.137757   
Customer_Service_Calls      -15.787590     -1.018250         -11.745841   

                        Promo_Conversions  Customer_Service_Calls  
Website_Visits                   3.553174              -15.787590  
Time_On_Site                     1.636443               -1.018250  
Newsletter_Clicks               45.137757              -11.745841  
Promo_Conversions               39.584051                2.624194  
Customer_Service_Calls           2.624194              162.939196  

Scaled Data Head (First 5 rows):
   Website_Visits  Time_On_Site  Newsletter_Clicks  Promo_Conversions  \
0        0.664619     -0.294759           0.271485          -0.373394   
1       -0.038089     -0.307890           0.459646          -0.045977   
2        0.831697      0.517729           0.943743           1.189920   
3        1.800406      1.099473           0.916631           1.080003   
4       -0.144206     -0.242396          -1.337135          -1.187735   

   Customer_Service_Calls  
0               -1.453539  
1               -0.513389  
2                0.057869  
3                0.097303  
4               -0.372318  

Scaled Data Covariance Matrix (demonstrates correlations on standardized scale):
                        Website_Visits  Time_On_Site  Newsletter_Clicks  \
Website_Visits                1.010101      0.822019           0.192768   
Time_On_Site                  0.822019      1.010101           0.150595   
Newsletter_Clicks             0.192768      0.150595           1.010101   
Promo_Conversions             0.062814      0.032322           0.835434   
Customer_Service_Calls       -0.137563     -0.009913          -0.107153   

                        Promo_Conversions  Customer_Service_Calls  
Website_Visits                   0.062814               -0.137563  
Time_On_Site                     0.032322               -0.009913  
Newsletter_Clicks                0.835434               -0.107153  
Promo_Conversions                1.010101                0.033006  
Customer_Service_Calls           0.033006                1.010101  

Explained Variance Ratio by each Principal Component: [0.41092429 0.32234883 0.20078359 0.03528015 0.03066314]
Cumulative Explained Variance: [0.41092429 0.73327312 0.93405671 0.96933686 1.        ]
</pre></div>
</div>
<img alt="../../_images/98ac2de1df61fb500116ba01ef0ab2647e0cc87d6a3d1309fa991bdfea5afe99.png" src="../../_images/98ac2de1df61fb500116ba01ef0ab2647e0cc87d6a3d1309fa991bdfea5afe99.png" />
<img alt="../../_images/af051b6c01df351eefacfcf741ad1fdb4a60e30c76e25967e7a769739b8a9eb6.png" src="../../_images/af051b6c01df351eefacfcf741ad1fdb4a60e30c76e25967e7a769739b8a9eb6.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Transformed Data (First 2 Principal Components) Head:
   Principal Component 1  Principal Component 2
0               0.329634              -0.347166
1               0.110852               0.325155
2               1.715961               0.431588
3               2.423823              -0.396480
4              -1.404664              -1.100460
</pre></div>
</div>
<img alt="../../_images/5b796abc4fda9c9c1b810ca05aca380350955e4f6ddef4caf6998a4631fd0917.png" src="../../_images/5b796abc4fda9c9c1b810ca05aca380350955e4f6ddef4caf6998a4631fd0917.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Eigenvectors (Loadings) for the first 2 Principal Components:
                             PC1       PC2
Website_Visits          0.511123 -0.481480
Time_On_Site            0.484428 -0.500822
Newsletter_Clicks       0.531243  0.459490
Promo_Conversions       0.458059  0.549556
Customer_Service_Calls -0.109755  0.064897
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Interpretation of Loadings:</span>
<span class="c1"># - For PC1, &#39;Website_Visits&#39; and &#39;Time_On_Site&#39; have high positive loadings,</span>
<span class="c1">#   suggesting PC1 represents a general &#39;Activity Level&#39;.</span>
<span class="c1"># - For PC2, &#39;Newsletter_Clicks&#39; and &#39;Promo_Conversions&#39; have high positive loadings,</span>
<span class="c1">#   suggesting PC2 represents &#39;Promotional Engagement&#39;.</span>
<span class="c1"># - &#39;Customer_Service_Calls&#39; might have lower loadings on these main PCs,</span>
<span class="c1">#   or higher on subsequent PCs if retained.</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Interpretation of PCA Plots:</strong></p>
<ul class="simple">
<li><p><strong>Scree Plot:</strong> This plot helps you decide how many principal components to keep. You’d look for an “elbow” in the graph. In our example, PC1 explains a large chunk, PC2 explains a bit less but still significant. After PC2, the drop in explained variance is less steep, suggesting that 2 or 3 components might be sufficient.</p></li>
<li><p><strong>Cumulative Explained Variance Plot:</strong> This plot directly shows the total percentage of variance explained by keeping a certain number of principal components. For instance, you might see that keeping 2 components explains 75% of the total variance, and 3 components explain 90%. This helps you balance dimensionality reduction with information retention.</p></li>
<li><p><strong>Data Projected onto First Two Principal Components Plot:</strong> This is a scatter plot of your data points in the new, lower-dimensional space defined by PC1 and PC2. This visualization is particularly useful for identifying clusters, trends, or outliers that might have been obscured in the original high-dimensional space. The axes are the principal components, and they are orthogonal, reflecting the uncorrelated nature of the new features.</p></li>
</ul>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="standardization-vs-normalization-understanding-their-impact-with-plots">
<h1>Standardization vs. Normalization: Understanding Their Impact with Plots<a class="headerlink" href="#standardization-vs-normalization-understanding-their-impact-with-plots" title="Link to this heading">#</a></h1>
<p>This notebook provides a detailed explanation of Standardization and Normalization, two crucial data preprocessing techniques. It visually demonstrates their effects on data distributions, particularly highlighting how they handle outliers.</p>
<section id="introduction-to-feature-scaling">
<h2>Introduction to Feature Scaling<a class="headerlink" href="#introduction-to-feature-scaling" title="Link to this heading">#</a></h2>
<p>In machine learning and statistical analysis, numerical features often come in different scales and units (e.g., age in years, income in thousands of dollars). Algorithms that rely on distance calculations (like K-Nearest Neighbors, Support Vector Machines, K-means clustering) or gradient descent (like Linear Regression, Neural Networks) can be heavily biased by features with larger scales, causing them to dominate the learning process.</p>
<p><strong>Feature scaling</strong> is the process of transforming numerical data into a common scale without distorting differences in the ranges of values. Standardization and Normalization are the two most common methods for this.</p>
</section>
<hr class="docutils" />
<section id="standardization-z-score-scaling">
<h2>1. Standardization (Z-score Scaling)<a class="headerlink" href="#standardization-z-score-scaling" title="Link to this heading">#</a></h2>
<p><strong>Explanation:</strong>
<strong>Standardization</strong> (also known as Z-score normalization) transforms data to have a <strong>mean of 0</strong> and a <strong>standard deviation of 1</strong>. This process centers the data around its mean and scales it by its variability.</p>
<p><strong>Formula:</strong>
For a data point <span class="math notranslate nohighlight">\(x\)</span>, its standardized value <span class="math notranslate nohighlight">\(z\)</span> is calculated as:
$<span class="math notranslate nohighlight">\(z = \frac{x - \mu}{\sigma}\)</span>$
Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x\)</span>: The original value of the data point.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mu\)</span>: The mean of the feature (column).</p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma\)</span>: The standard deviation of the feature (column).</p></li>
</ul>
<p><strong>Mathematical Intuition:</strong>
Standardization essentially measures how many standard deviations away from the mean a data point is. This makes all features contribute equally to the distance calculations in algorithms. It shifts the distribution so that its center is at 0 and scales its spread to 1. <strong>The shape of the original distribution is preserved.</strong></p>
<p><strong>When to Use Standardization:</strong></p>
<ul class="simple">
<li><p><strong>Algorithms sensitive to feature scales and/or assuming normality:</strong> Many machine learning algorithms, especially those that use distance calculations or assume a normal distribution, perform better with standardized data. Examples include:</p>
<ul>
<li><p><strong>K-Nearest Neighbors (KNN)</strong></p></li>
<li><p><strong>Support Vector Machines (SVM)</strong></p></li>
<li><p><strong>Linear and Logistic Regression</strong></p></li>
<li><p><strong>Neural Networks</strong> (can help with faster convergence)</p></li>
<li><p><strong>Principal Component Analysis (PCA)</strong>: PCA is sensitive to the variance of features. Standardizing ensures that features with larger scales don’t disproportionately influence the principal components.</p></li>
</ul>
</li>
<li><p><strong>Data with outliers:</strong> Standardization is generally more robust to outliers than Min-Max Normalization because it uses the mean and standard deviation, which are less affected by extreme values than the min and max. The outlier will still be an outlier, but its distance from the mean will be expressed in standard deviations.</p></li>
<li><p><strong>Data with a Gaussian (Normal) distribution:</strong> While not strictly required, standardization works well when the data is approximately normally distributed.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="normalization-min-max-scaling">
<h2>2. Normalization (Min-Max Scaling)<a class="headerlink" href="#normalization-min-max-scaling" title="Link to this heading">#</a></h2>
<p><strong>Explanation:</strong>
<strong>Normalization</strong> (most commonly <strong>Min-Max Scaling</strong>) rescales numerical features to a <strong>fixed, predefined range</strong>, typically between 0 and 1. It transforms the data by shifting the range of values so that the minimum value becomes 0 and the maximum value becomes 1.</p>
<p><strong>Formula (Min-Max Scaling):</strong>
For a data point <span class="math notranslate nohighlight">\(x\)</span>, its normalized value <span class="math notranslate nohighlight">\(x'\)</span> is calculated as:
$<span class="math notranslate nohighlight">\(x' = \frac{x - \text{min}(x)}{\text{max}(x) - \text{min}(x)}\)</span>$
Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x\)</span>: The original value of the data point.</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{min}(x)\)</span>: The minimum value of the feature (column).</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{max}(x)\)</span>: The maximum value of the feature (column).</p></li>
</ul>
<p><strong>Mathematical Intuition:</strong>
Normalization compresses the entire range of the data into a specific interval. It’s like fitting all your data points into a box of a specific size (e.g., 0 to 1). This is useful when you need to constrain the values within a certain boundary. <strong>The shape of the original distribution is preserved,</strong> but the scale is completely changed to fit the new range.</p>
<p><strong>When to Use Normalization:</strong></p>
<ul class="simple">
<li><p><strong>Algorithms that don’t assume a specific distribution:</strong></p>
<ul>
<li><p><strong>Neural Networks:</strong> Normalization (especially to [0,1] or [-1,1]) is often preferred as it can lead to faster convergence and better performance by preventing large input values from saturating activation functions.</p></li>
<li><p><strong>K-Nearest Neighbors (KNN):</strong> While standardization also works, normalization can be used if the absolute scale of features within a bounded range is important.</p></li>
</ul>
</li>
<li><p><strong>When you need values within a specific range:</strong> For example, image processing where pixel intensities are often normalized to [0, 1].</p></li>
<li><p><strong>When the data distribution is unknown or not Gaussian:</strong> Normalization makes no assumptions about the distribution of the data.</p></li>
<li><p><strong>When outliers are NOT a major concern:</strong> Normalization is highly sensitive to outliers. A single extreme outlier can compress the majority of the non-outlier data into a very small range (e.g., 0 to 0.1), reducing its variability and making it difficult for the model to distinguish between these compressed values.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="key-differences-visual-impact">
<h2>Key Differences &amp; Visual Impact<a class="headerlink" href="#key-differences-visual-impact" title="Link to this heading">#</a></h2>
<p>Let’s use a simulated dataset to visually understand the impact of these transformations. We’ll create a feature with a clear outlier to demonstrate sensitivity.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span><span class="p">,</span> <span class="n">MinMaxScaler</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">&quot;whitegrid&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.family&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;sans-serif&#39;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;font.sans-serif&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;DejaVu Sans&#39;</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;--- Visualizing Standardization vs. Normalization ---&quot;</span><span class="p">)</span>

<span class="c1"># 1. Create a sample dataset with an outlier</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;Feature_A&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="c1"># Normally distributed</span>
    <span class="s1">&#39;Feature_B&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="c1"># Uniformly distributed</span>
    <span class="s1">&#39;Feature_C_With_Outlier&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">95</span><span class="p">),</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">105</span><span class="p">,</span> <span class="mi">110</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">200</span><span class="p">]])</span> <span class="c1"># Data with a few outliers</span>
<span class="p">}</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original Data Head:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Original Data Descriptive Statistics:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>

<span class="c1"># --- Plotting Original Distributions ---</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Original Feature Distributions&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Feature_A&#39;</span><span class="p">],</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;skyblue&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Feature A (Normal)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Value&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Feature_B&#39;</span><span class="p">],</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;lightcoral&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Feature B (Uniform)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Value&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Feature_C_With_Outlier&#39;</span><span class="p">],</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;mediumseagreen&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Feature C (With Outliers)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Value&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">rect</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="c1"># --- Apply Standardization ---</span>
<span class="n">scaler_standard</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">df_standardized</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scaler_standard</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Standardized Data Head:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df_standardized</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Standardized Data Descriptive Statistics (Mean ~0, Std Dev ~1):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df_standardized</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>

<span class="c1"># --- Plotting Standardized Distributions ---</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Standardized Feature Distributions (Mean=0, Std Dev=1)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">df_standardized</span><span class="p">[</span><span class="s1">&#39;Feature_A&#39;</span><span class="p">],</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;skyblue&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Feature A (Standardized)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Z-score&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># Common range for standardized data</span>

<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">df_standardized</span><span class="p">[</span><span class="s1">&#39;Feature_B&#39;</span><span class="p">],</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;lightcoral&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Feature B (Standardized)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Z-score&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">df_standardized</span><span class="p">[</span><span class="s1">&#39;Feature_C_With_Outlier&#39;</span><span class="p">],</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;mediumseagreen&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Feature C (Standardized - Outliers Preserved)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Z-score&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="c1"># Notice outliers are still far from mean (0), maintaining their relative position</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># May need to adjust if outliers are extremely far</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">rect</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="c1"># --- Apply Normalization (Min-Max Scaling) ---</span>
<span class="n">scaler_minmax</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">df_normalized</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scaler_minmax</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Normalized Data Head:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df_normalized</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Normalized Data Descriptive Statistics (Min=0, Max=1):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df_normalized</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>

<span class="c1"># --- Plotting Normalized Distributions ---</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Normalized Feature Distributions (Min=0, Max=1)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">df_normalized</span><span class="p">[</span><span class="s1">&#39;Feature_A&#39;</span><span class="p">],</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;skyblue&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Feature A (Normalized)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Scaled Value&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span> <span class="c1"># Ensure range is visible</span>

<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">df_normalized</span><span class="p">[</span><span class="s1">&#39;Feature_B&#39;</span><span class="p">],</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;lightcoral&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Feature B (Normalized)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Scaled Value&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">df_normalized</span><span class="p">[</span><span class="s1">&#39;Feature_C_With_Outlier&#39;</span><span class="p">],</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;mediumseagreen&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Feature C (Normalized - Outliers Compress Data)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Scaled Value&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="c1"># Notice how the non-outlier data is squashed into a very small range near 0</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">rect</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>


<span class="c1"># --- Direct Comparison of Feature C (with Outlier) ---</span>
<span class="c1"># This plot clearly shows the impact of outliers on each method</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Impact of Scaling on Feature C (with Outliers)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>

<span class="c1"># Original</span>
<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Feature_C_With_Outlier&#39;</span><span class="p">],</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;mediumseagreen&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Original Feature C&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Original Value&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>

<span class="c1"># Standardized</span>
<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">df_standardized</span><span class="p">[</span><span class="s1">&#39;Feature_C_With_Outlier&#39;</span><span class="p">],</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;steelblue&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Standardized Feature C&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Z-score&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="c1"># Set x-limits to show the outlier&#39;s position relative to the bulk</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="n">df_standardized</span><span class="p">[</span><span class="s1">&#39;Feature_C_With_Outlier&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">df_standardized</span><span class="p">[</span><span class="s1">&#39;Feature_C_With_Outlier&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Mean (0)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>


<span class="c1"># Normalized</span>
<span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">df_normalized</span><span class="p">[</span><span class="s1">&#39;Feature_C_With_Outlier&#39;</span><span class="p">],</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;darkorange&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Normalized Feature C&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Scaled Value (0-1)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Frequency&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="c1"># Set x-limits to show the compression</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">df_normalized</span><span class="p">[</span><span class="s1">&#39;Feature_C_With_Outlier&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Min (0)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">df_normalized</span><span class="p">[</span><span class="s1">&#39;Feature_C_With_Outlier&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;purple&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Max (1)&#39;</span><span class="p">)</span>
<span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">rect</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">--- Summary of Differences ---&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;| Feature             | Standardization (Z-score Scaling)                               | Normalization (Min-Max Scaling)                                  |&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;| :------------------ | :-------------------------------------------------------------- | :--------------------------------------------------------------- |&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;| **Formula** | $z = (x - </span><span class="se">\\</span><span class="s2">mu) / </span><span class="se">\\</span><span class="s2">sigma$                                        | $x&#39; = (x - </span><span class="se">\\</span><span class="s2">text</span><span class="si">{min}</span><span class="s2">(x)) / (</span><span class="se">\\</span><span class="s2">text</span><span class="si">{max}</span><span class="s2">(x) - </span><span class="se">\\</span><span class="s2">text</span><span class="si">{min}</span><span class="s2">(x))$     |&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;| **Resulting Scale** | Mean = 0, Standard Deviation = 1                                | Fixed range, typically [0, 1] (or [-1, 1])                       |&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;| **Distribution** | Preserves the shape of the original distribution.              | Preserves the shape of the original distribution.                |&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;| **Outlier Sensitivity** | Less sensitive to outliers (uses mean &amp; std dev).             | Highly sensitive to outliers (uses min &amp; max, which are affected). |&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;| **Data Distribution Assumption** | Often preferred when data is approximately Gaussian/Normal. | No assumption about data distribution.                           |&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;| **Bounded Range** | Not bounded to a specific range (can have values outside [-3, 3]). | Bounded to a specific range (e.g., [0, 1]).                      |&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;| **Use Cases** | PCA, SVM, Linear/Logistic Regression, Neural Networks (can benefit). | Neural Networks (often preferred), image processing, algorithms that require bounded inputs. |&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">In practice, it&#39;s often a good idea to try both and see which one yields better performance for your specific machine learning model and dataset.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>--- Visualizing Standardization vs. Normalization ---
Original Data Head:
   Feature_A  Feature_B  Feature_C_With_Outlier
0  54.967142          9               10.217424
1  48.617357         62               10.211527
2  56.476885         37               11.841333
3  65.230299         97                9.546555
4  47.658466         51               11.306097

Original Data Descriptive Statistics:
        Feature_A   Feature_B  Feature_C_With_Outlier
count  100.000000  100.000000              100.000000
mean    48.961535   49.660000               15.915632
std      9.081684   31.001212               28.389140
min     23.802549    1.000000                4.241086
25%     43.990943   20.000000                8.238917
50%     48.730437   52.000000                9.990447
75%     54.059521   73.500000               11.708796
max     68.522782   99.000000              200.000000
</pre></div>
</div>
<img alt="../../_images/27261d66a4164036a3bf8906d9611658e91c5e9d2e92bc75313f1a3a08b0ffad.png" src="../../_images/27261d66a4164036a3bf8906d9611658e91c5e9d2e92bc75313f1a3a08b0ffad.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Standardized Data Head:
   Feature_A  Feature_B  Feature_C_With_Outlier
0   0.664619  -1.318169               -0.201729
1  -0.038089   0.400054               -0.201938
2   0.831697  -0.410428               -0.144239
3   1.800406   1.534730               -0.225479
4  -0.144206   0.043442               -0.163188

Standardized Data Descriptive Statistics (Mean ~0, Std Dev ~1):
          Feature_A     Feature_B  Feature_C_With_Outlier
count  1.000000e+02  1.000000e+02            1.000000e+02
mean  -1.156020e-15  1.072753e-16            4.440892e-17
std    1.005038e+00  1.005038e+00            1.005038e+00
min   -2.784256e+00 -1.577523e+00           -4.133045e-01
25%   -5.500777e-01 -9.615566e-01           -2.717725e-01
50%   -2.557477e-02  7.586118e-02           -2.097645e-01
75%    5.641760e-01  7.728763e-01           -1.489312e-01
max    2.164774e+00  1.599569e+00            6.516990e+00
</pre></div>
</div>
<img alt="../../_images/6905f47068a3831f0ee0590a5aa9bb923e947d564afad14819f4ab6a98bab00d.png" src="../../_images/6905f47068a3831f0ee0590a5aa9bb923e947d564afad14819f4ab6a98bab00d.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Normalized Data Head:
   Feature_A  Feature_B  Feature_C_With_Outlier
0   0.696879   0.081633                0.030529
1   0.554890   0.622449                0.030499
2   0.730639   0.367347                0.038825
3   0.926376   0.979592                0.027102
4   0.533448   0.510204                0.036090

Normalized Data Descriptive Statistics (Min=0, Max=1):
        Feature_A   Feature_B  Feature_C_With_Outlier
count  100.000000  100.000000              100.000000
mean     0.562586    0.496531                0.059637
std      0.203078    0.316339                0.145021
min      0.000000    0.000000                0.000000
25%      0.451438    0.193878                0.020422
50%      0.557419    0.520408                0.029370
75%      0.676583    0.739796                0.038147
max      1.000000    1.000000                1.000000
</pre></div>
</div>
<img alt="../../_images/84544f3542dac6bdea2366c7b5abdc25d10ecc881c2f47c9c97e87a760eb04cc.png" src="../../_images/84544f3542dac6bdea2366c7b5abdc25d10ecc881c2f47c9c97e87a760eb04cc.png" />
<img alt="../../_images/414af1b003040bba2522238fadb8d2b4b922c025d27f0ade4712685c821290bc.png" src="../../_images/414af1b003040bba2522238fadb8d2b4b922c025d27f0ade4712685c821290bc.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>--- Summary of Differences ---
| Feature             | Standardization (Z-score Scaling)                               | Normalization (Min-Max Scaling)                                  |
| :------------------ | :-------------------------------------------------------------- | :--------------------------------------------------------------- |
| **Formula** | $z = (x - \mu) / \sigma$                                        | $x&#39; = (x - \text{min}(x)) / (\text{max}(x) - \text{min}(x))$     |
| **Resulting Scale** | Mean = 0, Standard Deviation = 1                                | Fixed range, typically [0, 1] (or [-1, 1])                       |
| **Distribution** | Preserves the shape of the original distribution.              | Preserves the shape of the original distribution.                |
| **Outlier Sensitivity** | Less sensitive to outliers (uses mean &amp; std dev).             | Highly sensitive to outliers (uses min &amp; max, which are affected). |
| **Data Distribution Assumption** | Often preferred when data is approximately Gaussian/Normal. | No assumption about data distribution.                           |
| **Bounded Range** | Not bounded to a specific range (can have values outside [-3, 3]). | Bounded to a specific range (e.g., [0, 1]).                      |
| **Use Cases** | PCA, SVM, Linear/Logistic Regression, Neural Networks (can benefit). | Neural Networks (often preferred), image processing, algorithms that require bounded inputs. |

In practice, it&#39;s often a good idea to try both and see which one yields better performance for your specific machine learning model and dataset.
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./Mathematics for Data Science\Statistics"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">🔹 Principal Component Analysis (PCA)</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#standardization-vs-normalization-understanding-their-impact-with-plots">Standardization vs. Normalization: Understanding Their Impact with Plots</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-feature-scaling">Introduction to Feature Scaling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#standardization-z-score-scaling">1. Standardization (Z-score Scaling)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization-min-max-scaling">2. Normalization (Min-Max Scaling)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-differences-visual-impact">Key Differences &amp; Visual Impact</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>