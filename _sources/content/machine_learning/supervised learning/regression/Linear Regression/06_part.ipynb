{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e4e36b6",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e941143",
   "metadata": {},
   "source": [
    "## OLS\n",
    "\n",
    "* **OLS (Ordinary Least Squares)** is the most common method to estimate the parameters (coefficients) of a linear regression model.\n",
    "* It finds the **best-fit line** by **minimizing the sum of squared errors (residuals)** between actual and predicted values.\n",
    "\n",
    "$$\n",
    "\\text{Residual (error)} = y_i - \\hat{y}_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\beta_0 + \\beta_1x_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Objective\n",
    "\n",
    "OLS minimizes:\n",
    "\n",
    "$$\n",
    "SSE = \\sum_{i=1}^n (y_i - (\\beta_0 + \\beta_1x_i))^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $y_i$ = actual value\n",
    "* $\\hat{y}_i$ = predicted value\n",
    "* $n$ = number of observations\n",
    "\n",
    "This ensures the line is as close as possible to all data points.\n",
    "\n",
    "---\n",
    "\n",
    "### Derivation (Simple Linear Regression)\n",
    "\n",
    "We solve for parameters $\\beta_0$ (intercept) and $\\beta_1$ (slope) using calculus:\n",
    "\n",
    "1. Take partial derivatives of SSE wrt $\\beta_0$ and $\\beta_1$.\n",
    "2. Set them = 0 (to minimize error).\n",
    "3. Solve â†’ gives the **normal equations**.\n",
    "\n",
    "Final formulas:\n",
    "\n",
    "* **Slope**:\n",
    "\n",
    "$$\n",
    "\\beta_1 = \\frac{\\sum (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum (x_i - \\bar{x})^2}\n",
    "$$\n",
    "\n",
    "* **Intercept**:\n",
    "\n",
    "$$\n",
    "\\beta_0 = \\bar{y} - \\beta_1 \\bar{x}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Multiple Linear Regression (Matrix Form)\n",
    "\n",
    "For multiple features:\n",
    "\n",
    "$$\n",
    "Y = X\\beta + \\epsilon\n",
    "$$\n",
    "\n",
    "OLS solution:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = (X^TX)^{-1}X^TY\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $X$ = feature matrix\n",
    "* $Y$ = target vector\n",
    "* $\\beta$ = coefficient vector\n",
    "\n",
    "---\n",
    "\n",
    "### Why OLS?\n",
    "\n",
    "âœ… Simple and widely used\n",
    "âœ… Provides exact solution (no iterations needed, unlike Gradient Descent)\n",
    "âœ… Works well when data assumptions hold (linearity, independence, homoscedasticity, normality of errors)\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ‘‰ In short:\n",
    "OLS gives us a **mathematical way** to find the regression line by minimizing squared differences between actual and predicted values.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
