{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6d43255",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0499e9c6",
   "metadata": {},
   "source": [
    "# Matrix Decomposition\n",
    "\n",
    "In **linear algebra**, **decomposition** or **factorization** refers to expressing a matrix as a product of simpler matrices. These factorizations are fundamental because they reveal the structure of the original matrix, simplify computations, and provide insight into how matrices transform vectors and define vector spaces.\n",
    "\n",
    "The term “vector decomposition” can also refer to expressing vectors in terms of basis vectors, or understanding how matrix factorizations describe vector transformations within different vector spaces.\n",
    "\n",
    "\n",
    "## LU Decomposition (A = LU)\n",
    "\n",
    "**Concept:**\n",
    "LU decomposition expresses a matrix `A` as the product of a **Lower triangular matrix L** and an **Upper triangular matrix U**:\n",
    "\n",
    "$$\n",
    "A = LU \\quad \\text{or} \\quad PA = LU \\text{ (if row exchanges are needed)}\n",
    "$$\n",
    "\n",
    "* **L (Lower Triangular):** Contains the multipliers used to eliminate entries below the diagonal in Gaussian elimination.\n",
    "* **U (Upper Triangular):** Essentially the echelon form of `A`.\n",
    "\n",
    "**Why it matters:**\n",
    "\n",
    "* When solving `Ax = b`, instead of inverting `A`, we can solve two simpler triangular systems:\n",
    "\n",
    "  1. `Lc = b` (forward substitution)\n",
    "  2. `Ux = c` (back substitution)\n",
    "\n",
    "* **Vector perspective:**\n",
    "  Each step of elimination corresponds to subtracting multiples of one vector (row) from another. LU decomposition reveals how the rows of `A` relate linearly.\n",
    "\n",
    "**Example Use Case:**\n",
    "\n",
    "* Repeatedly solving `Ax = b` for multiple `b`s is much faster using LU decomposition.\n",
    "\n",
    "---\n",
    "\n",
    "## QR Decomposition (A = QR)\n",
    "\n",
    "**Concept:**\n",
    "QR decomposition writes a matrix as:\n",
    "\n",
    "$$\n",
    "A = QR\n",
    "$$\n",
    "\n",
    "* **Q (Orthogonal / Orthonormal):** Columns form an orthonormal basis for the column space of `A`.\n",
    "* **R (Upper Triangular):** Captures the linear combinations that express the original columns in terms of Q.\n",
    "\n",
    "**Why it matters:**\n",
    "\n",
    "* **Least-squares problems:** If `Ax ≈ b` has no exact solution, QR simplifies the normal equations `AᵀAx̂ = Aᵀb` to `Rx̂ = Qᵀb`.\n",
    "* **Vector perspective:**\n",
    "\n",
    "  * Q represents an orthonormal “coordinate system” for the column space of `A`.\n",
    "  * Each column of `A` is expressed as a linear combination of the columns of Q.\n",
    "* **Geometric interpretation:** QR decomposition can be seen as rotating and scaling vectors to align with an orthonormal basis.\n",
    "\n",
    "---\n",
    "\n",
    "## Singular Value Decomposition (SVD) (A = UΣVᵀ)\n",
    "\n",
    "**Concept:**\n",
    "SVD factorizes any matrix (square or rectangular) into:\n",
    "\n",
    "$$\n",
    "A = U \\Sigma V^\\top\n",
    "$$\n",
    "\n",
    "* **U:** Orthonormal basis for the column space (left singular vectors).\n",
    "* **Σ:** Diagonal matrix with singular values (scales).\n",
    "* **Vᵀ:** Orthonormal basis for the row space (right singular vectors).\n",
    "\n",
    "**Why it matters:**\n",
    "\n",
    "* **Rank and subspaces:** SVD clearly separates `A`’s action into directions it stretches (singular values) and directions it maps.\n",
    "* **Vector perspective:**\n",
    "\n",
    "  * Each vector in the row space (`vᵢ`) is mapped to the column space (`σᵢ uᵢ`).\n",
    "  * Reveals which directions are amplified or diminished.\n",
    "* **Applications:**\n",
    "\n",
    "  * Pseudoinverse: Solving `Ax = b` when `A` is non-square or rank-deficient.\n",
    "  * Data compression: Keep largest singular values to approximate `A`.\n",
    "  * Dimensionality reduction in machine learning (PCA).\n",
    "\n",
    "---\n",
    "\n",
    "## Eigenvalue Decomposition (A = SΛS⁻¹)\n",
    "\n",
    "**Concept:**\n",
    "For a square matrix `A`, if it has `n` independent eigenvectors:\n",
    "\n",
    "$$\n",
    "A = S \\Lambda S^{-1}\n",
    "$$\n",
    "\n",
    "* **S:** Columns are eigenvectors of `A`.\n",
    "* **Λ:** Diagonal matrix of eigenvalues.\n",
    "\n",
    "**Why it matters:**\n",
    "\n",
    "* Powers of `A`: `Aᵏ = SΛᵏS⁻¹`.\n",
    "* **Vector perspective:** Eigenvectors are special directions: `Av = λv`. They don’t change direction, only scale.\n",
    "* **Symmetric matrices:** `A = QΛQᵀ` with orthonormal eigenvectors → real eigenvalues and preserved angles/lengths.\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "* Solving differential/difference equations.\n",
    "* Principal component analysis (PCA) in statistics.\n",
    "* Understanding stability and dynamics in systems.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Cholesky Decomposition (A = CCᵀ or LDLᵀ)\n",
    "\n",
    "**Concept:**\n",
    "For **symmetric positive definite matrices**, decompose:\n",
    "\n",
    "$$\n",
    "A = CC^\\top\n",
    "$$\n",
    "\n",
    "* **C:** Lower triangular matrix.\n",
    "\n",
    "**Why it matters:**\n",
    "\n",
    "* Solves `Ax = b` efficiently.\n",
    "* Numerically stable because it avoids subtractive cancellation.\n",
    "* **Vector perspective:** Decomposes the quadratic form `xᵀAx` into squared contributions along orthogonal directions.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Polar Decomposition (A = QS)\n",
    "\n",
    "**Concept:**\n",
    "Any square matrix `A` can be decomposed as:\n",
    "\n",
    "$$\n",
    "A = Q S\n",
    "$$\n",
    "\n",
    "* **Q:** Orthogonal (rotation/reflection)\n",
    "* **S:** Symmetric positive semidefinite (stretching)\n",
    "\n",
    "**Vector perspective:**\n",
    "\n",
    "* Shows how `A` transforms a vector: first rotates (Q) and then stretches (S).\n",
    "* Useful in physics, robotics, and continuum mechanics.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Jordan Form (A = MJM⁻¹)\n",
    "\n",
    "**Concept:**\n",
    "Jordan decomposition represents `A` in a near-diagonal form, even if not diagonalizable:\n",
    "\n",
    "$$\n",
    "A = M J M^{-1}\n",
    "$$\n",
    "\n",
    "* **J:** Jordan blocks with eigenvalues on the diagonal, ones on superdiagonal if eigenvectors are missing.\n",
    "\n",
    "**Vector perspective:**\n",
    "\n",
    "* Reveals generalized eigenvectors, showing how vectors are transformed in defective directions.\n",
    "\n",
    "**Use:** Computing `Aᵏ` or `exp(A)` for non-diagonalizable matrices.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Schur Decomposition (U⁻¹AU = T)\n",
    "\n",
    "**Concept:**\n",
    "Any square matrix can be written as:\n",
    "\n",
    "$$\n",
    "U^{-1} A U = T\n",
    "$$\n",
    "\n",
    "* **T:** Upper triangular with eigenvalues on the diagonal\n",
    "* **U:** Unitary matrix (orthonormal columns)\n",
    "\n",
    "**Vector perspective:**\n",
    "\n",
    "* Transforms vectors to a basis where the action of `A` is almost triangular (upper triangular).\n",
    "* Useful in numerical eigenvalue computations.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Fast Fourier Transform (FFT)\n",
    "\n",
    "**Concept:**\n",
    "FFT is an efficient factorization of the **Fourier matrix** used in Discrete Fourier Transform:\n",
    "\n",
    "$$\n",
    "y = F_n x\n",
    "$$\n",
    "\n",
    "* Factorizes `F_n` into sparse matrices to reduce computations from `O(n²)` to `O(n log n)`.\n",
    "\n",
    "**Vector perspective:**\n",
    "\n",
    "* Efficiently transforms time-domain vectors into frequency-domain vectors.\n",
    "* Each basis vector in time maps to a frequency component in the Fourier basis.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of Vector Relevance Across Decompositions\n",
    "\n",
    "| Decomposition | Vector Perspective                                                 |\n",
    "| ------------- | ------------------------------------------------------------------ |\n",
    "| LU            | Reveals row interactions; simplifies solving `Ax = b`              |\n",
    "| QR            | Produces orthonormal basis for column space; solves least squares  |\n",
    "| SVD           | Maps orthonormal vectors in row space to column space with scaling |\n",
    "| Eigen         | Shows invariant directions (eigenvectors) scaled by eigenvalues    |\n",
    "| Cholesky      | Decomposes quadratic forms into orthogonal contributions           |\n",
    "| Polar         | Separates rotation and stretch applied to vectors                  |\n",
    "| Jordan        | Reveals generalized directions when eigenvectors are insufficient  |\n",
    "| Schur         | Triangularizes action of `A` on a new orthonormal basis            |\n",
    "| FFT           | Projects vectors onto Fourier basis (frequency components)         |\n",
    "\n",
    "---\n",
    "\n",
    "**Key Insight:**\n",
    "Matrix decompositions are not just about computation—they reveal **how a matrix acts on vectors**. They expose the structure of vector spaces (column space, row space, nullspace), define orthonormal or special bases (Q, U, V, eigenvectors), and simplify understanding transformations like rotation, stretching, projection, and scaling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af29534f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
